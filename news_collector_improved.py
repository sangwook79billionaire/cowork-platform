#!/usr/bin/env python3
"""
ê°œì„ ëœ ë‰´ìŠ¤ ìˆ˜ì§‘ê¸° - ë‹¤ì–‘í•œ ë‰´ìŠ¤ ì†ŒìŠ¤ì—ì„œ ë‰´ìŠ¤ë¥¼ ìˆ˜ì§‘í•˜ì—¬ ì—‘ì…€ë¡œ ì €ì¥í•˜ê³  Firebaseì— ì—…ë¡œë“œ
"""

import os
import json
import requests
import pandas as pd
from datetime import datetime, timedelta
from typing import List, Dict, Any
import firebase_admin
from firebase_admin import credentials, firestore
from dotenv import load_dotenv
import hashlib
import re
import time
import random

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

class ImprovedNewsCollector:
    def __init__(self):
        """ê°œì„ ëœ ë‰´ìŠ¤ ìˆ˜ì§‘ê¸° ì´ˆê¸°í™”"""
        self.collected_news = []
        self.firebase_db = None
        self._init_firebase()
    
    def _init_firebase(self):
        """Firebase ì´ˆê¸°í™”"""
        try:
            # Firebase Admin SDK ì´ˆê¸°í™”
            if not firebase_admin._apps:
                cred = credentials.Certificate("firebase/serviceAccountKey.json")
                firebase_admin.initialize_app(cred)
            
            self.firebase_db = firestore.client()
            print("âœ… Firebase ì—°ê²° ì„±ê³µ")
        except Exception as e:
            print(f"âŒ Firebase ì—°ê²° ì‹¤íŒ¨: {e}")
            self.firebase_db = None
    
    def generate_mock_news(self, keywords: str) -> List[Dict[str, Any]]:
        """ëª¨ì˜ ë‰´ìŠ¤ ë°ì´í„° ìƒì„± (ì‹¤ì œ ë‰´ìŠ¤ APIê°€ ì‘ë™í•˜ì§€ ì•Šì„ ë•Œ ì‚¬ìš©)"""
        mock_news = [
            {
                'title': f'{keywords} ê´€ë ¨ ìµœì‹  ê¸°ìˆ  ë™í–¥',
                'description': f'{keywords} ë¶„ì•¼ì—ì„œ í˜ì‹ ì ì¸ ë°œì „ì´ ì´ë£¨ì–´ì§€ê³  ìˆìŠµë‹ˆë‹¤. ì „ë¬¸ê°€ë“¤ì€ ì´ë²ˆ ê¸°ìˆ  ë°œì „ì´ ì‚°ì—… ì „ë°˜ì— í° ì˜í–¥ì„ ë¯¸ì¹  ê²ƒìœ¼ë¡œ ì˜ˆìƒí•©ë‹ˆë‹¤.',
                'content': f'{keywords} ê¸°ìˆ ì˜ ë°œì „ìœ¼ë¡œ ì¸í•´ ë‹¤ì–‘í•œ ì‚°ì—… ë¶„ì•¼ì—ì„œ ë³€í™”ê°€ ì¼ì–´ë‚˜ê³  ìˆìŠµë‹ˆë‹¤. íŠ¹íˆ AIì™€ ë¨¸ì‹ ëŸ¬ë‹ ê¸°ìˆ ì˜ ìœµí•©ìœ¼ë¡œ ìƒˆë¡œìš´ ê°€ëŠ¥ì„±ì´ ì—´ë¦¬ê³  ìˆìŠµë‹ˆë‹¤.',
                'url': f'https://example.com/news/{keywords.lower()}-latest',
                'source': {'name': 'ê¸°ìˆ ë‰´ìŠ¤', 'id': 'tech-news'},
                'category': 'technology'
            },
            {
                'title': f'{keywords} ì‹œì¥ ë™í–¥ ë¶„ì„',
                'description': f'{keywords} ì‹œì¥ì—ì„œ ìƒˆë¡œìš´ ë³€í™”ê°€ ê°ì§€ë˜ê³  ìˆìŠµë‹ˆë‹¤. íˆ¬ììë“¤ì€ ì´ë²ˆ ë³€í™”ì— ì£¼ëª©í•˜ê³  ìˆìŠµë‹ˆë‹¤.',
                'content': f'{keywords} ì‹œì¥ì˜ ë³€í™”ëŠ” ê¸€ë¡œë²Œ ê²½ì œì— ì˜í–¥ì„ ë¯¸ì¹  ê²ƒìœ¼ë¡œ ì˜ˆìƒë©ë‹ˆë‹¤. ì „ë¬¸ê°€ë“¤ì€ ì‹ ì¤‘í•œ ì ‘ê·¼ì´ í•„ìš”í•˜ë‹¤ê³  ì¡°ì–¸í•©ë‹ˆë‹¤.',
                'url': f'https://example.com/market/{keywords.lower()}-analysis',
                'source': {'name': 'ê²½ì œë‰´ìŠ¤', 'id': 'economy-news'},
                'category': 'economy'
            },
            {
                'title': f'{keywords} ì •ì±… ë³€í™” ì˜ˆê³ ',
                'description': f'{keywords} ê´€ë ¨ ì •ì±…ì´ ê³§ ë°œí‘œë  ì˜ˆì •ì…ë‹ˆë‹¤. ì´ë²ˆ ì •ì±… ë³€í™”ëŠ” ì‚°ì—…ê³„ì— í° ì˜í–¥ì„ ë¯¸ì¹  ê²ƒìœ¼ë¡œ ì˜ˆìƒë©ë‹ˆë‹¤.',
                'content': f'{keywords} ì •ì±…ì˜ ë³€í™”ëŠ” ê¸°ì—…ë“¤ì˜ ì „ëµ ìˆ˜ì •ì„ ìš”êµ¬í•  ê²ƒìœ¼ë¡œ ë³´ì…ë‹ˆë‹¤. ì •ë¶€ëŠ” ì´ë²ˆ ì •ì±…ì´ ê²½ì œ í™œì„±í™”ì— ë„ì›€ì´ ë  ê²ƒì´ë¼ê³  ë°í˜”ìŠµë‹ˆë‹¤.',
                'url': f'https://example.com/policy/{keywords.lower()}-changes',
                'source': {'name': 'ì •ì±…ë‰´ìŠ¤', 'id': 'policy-news'},
                'category': 'politics'
            },
            {
                'title': f'{keywords} êµìœ¡ í˜ì‹  ì‚¬ë¡€',
                'description': f'{keywords} ê¸°ìˆ ì„ í™œìš©í•œ êµìœ¡ í˜ì‹  ì‚¬ë¡€ê°€ ì£¼ëª©ë°›ê³  ìˆìŠµë‹ˆë‹¤. í•™ìƒë“¤ì˜ í•™ìŠµ íš¨ê³¼ê°€ í¬ê²Œ í–¥ìƒë˜ì—ˆë‹¤ê³  í•©ë‹ˆë‹¤.',
                'content': f'{keywords} ê¸°ìˆ ì˜ êµìœ¡ ì ìš©ì€ ìƒˆë¡œìš´ í•™ìŠµ ë°©ì‹ì„ ë§Œë“¤ì–´ë‚´ê³  ìˆìŠµë‹ˆë‹¤. êµì‚¬ë“¤ê³¼ í•™ìƒë“¤ ëª¨ë‘ ê¸ì •ì ì¸ ë°˜ì‘ì„ ë³´ì´ê³  ìˆìŠµë‹ˆë‹¤.',
                'url': f'https://example.com/education/{keywords.lower()}-innovation',
                'source': {'name': 'êµìœ¡ë‰´ìŠ¤', 'id': 'education-news'},
                'category': 'education'
            },
            {
                'title': f'{keywords} í™˜ê²½ ì˜í–¥ ì—°êµ¬',
                'description': f'{keywords} ê¸°ìˆ ì´ í™˜ê²½ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì— ëŒ€í•œ ì—°êµ¬ê°€ ì§„í–‰ë˜ê³  ìˆìŠµë‹ˆë‹¤. ì¹œí™˜ê²½ ê¸°ìˆ  ê°œë°œì˜ ì¤‘ìš”ì„±ì´ ê°•ì¡°ë˜ê³  ìˆìŠµë‹ˆë‹¤.',
                'content': f'{keywords} ê¸°ìˆ ì˜ í™˜ê²½ ì˜í–¥ì€ ì§€ì†ê°€ëŠ¥í•œ ë°œì „ì„ ìœ„í•´ ì¤‘ìš”í•œ ê³ ë ¤ì‚¬í•­ì…ë‹ˆë‹¤. ì—°êµ¬íŒ€ì€ ê¸ì •ì ì¸ ê²°ê³¼ë¥¼ ê¸°ëŒ€í•˜ê³  ìˆìŠµë‹ˆë‹¤.',
                'url': f'https://example.com/environment/{keywords.lower()}-impact',
                'source': {'name': 'í™˜ê²½ë‰´ìŠ¤', 'id': 'environment-news'},
                'category': 'environment'
            }
        ]
        
        # í‚¤ì›Œë“œë³„ë¡œ ë‹¤ì–‘í•œ ë³€í˜• ìƒì„±
        variations = [
            f'{keywords} ìµœì‹  ë™í–¥',
            f'{keywords} ì‹œì¥ ë¶„ì„',
            f'{keywords} ê¸°ìˆ  ë°œì „',
            f'{keywords} ì •ì±… ë³€í™”',
            f'{keywords} ì‚°ì—… ì˜í–¥'
        ]
        
        articles = []
        for i, news in enumerate(mock_news):
            # ì œëª©ì— ë³€í˜• ì ìš©
            news['title'] = news['title'].replace(f'{keywords} ê´€ë ¨', variations[i % len(variations)])
            
            # ê³ ìœ  ID ìƒì„±
            news_id = f"mock-{hashlib.md5((news['title'] + news['url']).encode()).hexdigest()[:8]}"
            
            article = {
                'id': news_id,
                'title': news['title'],
                'description': news['description'],
                'content': news['content'],
                'url': news['url'],
                'publishedAt': datetime.now().isoformat(),
                'source': news['source'],
                'keywords': keywords,
                'collectedAt': datetime.now().isoformat(),
                'category': news['category'],
                'language': 'ko',
                'isSaved': False,
                'savedBy': [],
                'viewCount': random.randint(0, 100)
            }
            
            articles.append(article)
        
        return articles
    
    def search_news(self, keywords: str, language: str = "ko") -> List[Dict[str, Any]]:
        """ë‰´ìŠ¤ ê²€ìƒ‰ (ëª¨ì˜ ë°ì´í„° ì‚¬ìš©)"""
        try:
            print(f"ğŸ” ë‰´ìŠ¤ ê²€ìƒ‰ ì‹œì‘: {keywords}")
            
            # ì‹¤ì œ ë‰´ìŠ¤ API ì‹œë„ (í˜„ì¬ëŠ” ëª¨ì˜ ë°ì´í„° ì‚¬ìš©)
            # TODO: ì‹¤ì œ ë‰´ìŠ¤ API ì—°ë™
            articles = self.generate_mock_news(keywords)
            
            print(f"âœ… {len(articles)}ê°œì˜ ë‰´ìŠ¤ë¥¼ ìˆ˜ì§‘í–ˆìŠµë‹ˆë‹¤.")
            return articles
            
        except Exception as e:
            print(f"âŒ ë‰´ìŠ¤ ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
            return []
    
    def remove_duplicates(self, articles: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """ì¤‘ë³µ ì œê±°"""
        seen_titles = set()
        unique_articles = []
        
        for article in articles:
            # ì œëª© ì •ê·œí™” (íŠ¹ìˆ˜ë¬¸ì ì œê±°, ì†Œë¬¸ì ë³€í™˜)
            normalized_title = re.sub(r'[^\w\s]', '', article['title'].lower())
            
            if normalized_title not in seen_titles:
                seen_titles.add(normalized_title)
                unique_articles.append(article)
        
        print(f"ğŸ”„ ì¤‘ë³µ ì œê±°: {len(articles)} â†’ {len(unique_articles)}")
        return unique_articles
    
    def save_to_excel(self, articles: List[Dict[str, Any]], filename: str = None) -> str:
        """ë‰´ìŠ¤ ë°ì´í„°ë¥¼ ì—‘ì…€ íŒŒì¼ë¡œ ì €ì¥"""
        if not filename:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"news_data_{timestamp}.xlsx"
        
        try:
            # DataFrame ìƒì„±
            df_data = []
            for article in articles:
                df_data.append({
                    'ID': article['id'],
                    'ì œëª©': article['title'],
                    'ì„¤ëª…': article['description'],
                    'ë‚´ìš©': article['content'],
                    'URL': article['url'],
                    'ì¶œì²˜': article['source']['name'],
                    'ì¹´í…Œê³ ë¦¬': article.get('category', ''),
                    'í‚¤ì›Œë“œ': article['keywords'],
                    'ìˆ˜ì§‘ì¼ì‹œ': article['collectedAt'],
                    'ì¡°íšŒìˆ˜': article.get('viewCount', 0)
                })
            
            df = pd.DataFrame(df_data)
            
            # ì—‘ì…€ íŒŒì¼ë¡œ ì €ì¥
            df.to_excel(filename, index=False, engine='openpyxl')
            print(f"âœ… ì—‘ì…€ íŒŒì¼ ì €ì¥ ì™„ë£Œ: {filename}")
            
            return filename
            
        except Exception as e:
            print(f"âŒ ì—‘ì…€ ì €ì¥ ì˜¤ë¥˜: {e}")
            return None
    
    def upload_to_firebase(self, articles: List[Dict[str, Any]], collection_name: str = "news") -> bool:
        """Firebase Firestoreì— ë‰´ìŠ¤ ë°ì´í„° ì—…ë¡œë“œ"""
        if not self.firebase_db:
            print("âŒ Firebase ì—°ê²°ì´ ì—†ìŠµë‹ˆë‹¤.")
            return False
        
        try:
            batch = self.firebase_db.batch()
            
            for article in articles:
                # ë¬¸ì„œ ID ìƒì„±
                doc_id = article['id']
                
                # Firestore ë¬¸ì„œ ì°¸ì¡°
                doc_ref = self.firebase_db.collection(collection_name).document(doc_id)
                
                # ë°°ì¹˜ì— ì¶”ê°€
                batch.set(doc_ref, article)
            
            # ë°°ì¹˜ ì»¤ë°‹
            batch.commit()
            
            print(f"âœ… Firebase ì—…ë¡œë“œ ì™„ë£Œ: {len(articles)}ê°œ ë¬¸ì„œ")
            return True
            
        except Exception as e:
            print(f"âŒ Firebase ì—…ë¡œë“œ ì˜¤ë¥˜: {e}")
            return False
    
    def collect_news(self, keywords: List[str], save_excel: bool = True, upload_firebase: bool = True) -> Dict[str, Any]:
        """ë‰´ìŠ¤ ìˆ˜ì§‘ ë©”ì¸ í•¨ìˆ˜"""
        all_articles = []
        
        for keyword in keywords:
            print(f"\nğŸ” í‚¤ì›Œë“œ '{keyword}' ê²€ìƒ‰ ì¤‘...")
            articles = self.search_news(keyword)
            all_articles.extend(articles)
            
            # API í˜¸ì¶œ ê°„ê²© ì¡°ì ˆ
            time.sleep(1)
        
        # ì¤‘ë³µ ì œê±°
        unique_articles = self.remove_duplicates(all_articles)
        
        # ê²°ê³¼ ì €ì¥
        result = {
            'total_collected': len(all_articles),
            'total_unique': len(unique_articles),
            'keywords': keywords,
            'excel_file': None,
            'firebase_uploaded': False
        }
        
        # ì—‘ì…€ ì €ì¥
        if save_excel and unique_articles:
            excel_file = self.save_to_excel(unique_articles)
            result['excel_file'] = excel_file
        
        # Firebase ì—…ë¡œë“œ
        if upload_firebase and unique_articles:
            firebase_success = self.upload_to_firebase(unique_articles)
            result['firebase_uploaded'] = firebase_success
        
        return result

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸš€ ê°œì„ ëœ ë‰´ìŠ¤ ìˆ˜ì§‘ê¸° ì‹œì‘")
    
    # ìˆ˜ì§‘í•  í‚¤ì›Œë“œ ëª©ë¡
    keywords = ["AI", "ì¸ê³µì§€ëŠ¥", "ê¸°ìˆ ", "ê²½ì œ", "ì£¼ì‹", "ë¶€ë™ì‚°", "ë¸”ë¡ì²´ì¸", "ë©”íƒ€ë²„ìŠ¤"]
    
    # ë‰´ìŠ¤ ìˆ˜ì§‘ê¸° ì´ˆê¸°í™”
    collector = ImprovedNewsCollector()
    
    # ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹¤í–‰
    result = collector.collect_news(
        keywords=keywords,
        save_excel=True,
        upload_firebase=True
    )
    
    # ê²°ê³¼ ì¶œë ¥
    print(f"\nğŸ“Š ìˆ˜ì§‘ ê²°ê³¼:")
    print(f"  - ì´ ìˆ˜ì§‘: {result['total_collected']}ê°œ")
    print(f"  - ì¤‘ë³µ ì œê±° í›„: {result['total_unique']}ê°œ")
    print(f"  - í‚¤ì›Œë“œ: {', '.join(result['keywords'])}")
    print(f"  - ì—‘ì…€ íŒŒì¼: {result['excel_file']}")
    print(f"  - Firebase ì—…ë¡œë“œ: {'ì„±ê³µ' if result['firebase_uploaded'] else 'ì‹¤íŒ¨'}")
    
    print("\nâœ… ë‰´ìŠ¤ ìˆ˜ì§‘ ì™„ë£Œ!")

if __name__ == "__main__":
    main() 